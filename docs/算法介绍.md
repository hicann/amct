# 算法介绍

## AWQ

AWQ（Activation-aware Weight Quantization，感知激活权重量化）是一种针对大模型的训练后量化（Post-Training Quantization，简称PTQ）方法，通过感知激活的权重量化策略，在保持模型精度的同时显著降低内存占用并提升推理速度。该算法认为并非所有权重对量化误差的敏感度都相同，仅保护约1%的显著权重即可大幅减少误差。使用少量校准数据，采用网格搜索在预设范围内寻找最优缩放因子，对显著权重进行放大后再量化，通过缩小量化间隔降低误差。

算法详细介绍请参见[Link](https://arxiv.org/abs/2306.00978)。

## GPTQ

GPTQ（Generative Pre-trained Transformer Quantization）是一种针对大模型的仅权重训练后量化（PTQ）方法，核心思路是逐层贪心量化，通过数学优化最小化量化误差。针对每个量化算子，根据激活值计算海森矩阵，分块将权重进行微调，最小化量化误差，最终输出调整后的权重以及量化因子。

算法详细介绍请参见[Link](https://arxiv.org/abs/2210.17323)。

## MIN-MAX

该算法是统计数据（activation）和权重（weight）中数据的最大值最小值，根据最大值和最小值，将数据缩放到量化位宽的边界值范围，然后根据量化公式计算出对应的缩放因子scale与偏移值offset。

在初始化时，根据权重量化颗粒度（比如PER_TENSOR）离线计算weight的最大值和最小值，离线计算得到scale_w/offset_w，添加到record对象中。前向传播时，获取当前batch的最大最小值，并刷新全局最大最小值。获取weight和activation的min/max值后，根据下面公式，计算scale和offset（对于weight，只支持对称场景，offset为0）。

- 对称量化场景

  ```
  if abs(max) > abs(min):
      scale = abs(max)/(2^(quant_bit-1)-1)
  else:
      scale = abs(max)/(2^(quant_bit-1))
      
  offset = 0
  ```

- 非对称量化场景

  ```
  scale=(max-min)/(2^quant_bit-1)
  
  offset=-2^(quant_bit-1)-min/scale
  ```

## SmoothQuant

SmoothQuant是一种同时确保准确率且推理高效的训练后量化 (PTQ) 方法，由于权重很容易量化，而激活则较难量化，SmoothQuant通过引入平滑系数来平滑激活异常值，通过数学上等效的变换将量化难度从激活转移到权重上。一般来说，激活异常值越多，迁移强度越大。

算法详细介绍请参见[Link](https://arxiv.org/abs/2211.10438)。



